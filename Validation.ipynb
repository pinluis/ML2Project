{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation\n",
    "\n",
    "After training both models, we will perform validation and comparison of the results.\n",
    "First we start by loading the models and the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "\n",
    "# Load GPT-2 Model\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2_finetuned_shakespeare')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define LSTM Model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embedded = self.embedding(input)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        out = self.fc(lstm_out)\n",
    "        return out\n",
    "\n",
    "# Define necessary parameters\n",
    "VOCAB_SIZE = len(tokenizer.get_vocab())  # You need to define the tokenizer\n",
    "EMBEDDING_DIM = 256\n",
    "HIDDEN_DIM = 512\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# Load LSTM Model\n",
    "lstm_model = LSTMModel(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM).to(device)\n",
    "lstm_model.load_state_dict(torch.load(\"lstm_model.pth\", map_location=torch.device('cpu')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\luisp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load the text\n",
    "with open('text.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Split the text into sentences\n",
    "sentences = nltk.tokenize.sent_tokenize(text)\n",
    "\n",
    "# Split sentences into training and validation sets\n",
    "train_sentences, val_sentences = train_test_split(sentences, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Create the validation dataset\n",
    "val_dataset = ShakespeareDataset(val_sentences, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, sentences, tokenizer):\n",
    "        self.sentences = sentences\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "        inputs = self.tokenizer(sentence, return_tensors='pt', truncation=True, padding='max_length', max_length=100)\n",
    "        input_ids = inputs[\"input_ids\"].squeeze().to(device)\n",
    "        attention_mask = inputs[\"attention_mask\"].squeeze().to(device)\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
    "\n",
    "# Tokenize the validation sentences and create the validation dataset\n",
    "val_dataset = ShakespeareDataset(val_sentences, tokenizer)\n",
    "\n",
    "# Create the DataLoader for the validation set\n",
    "BATCH_SIZE = 32\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a function to calculate the validation loss of both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_loss = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\", position=0, leave=True):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"input_ids\"].to(device)\n",
    "\n",
    "            if isinstance(model, LSTMModel):\n",
    "                outputs = model(input_ids)\n",
    "                loss = criterion(outputs.view(-1, VOCAB_SIZE), labels.view(-1))\n",
    "            else:  # For GPT2LMHeadModel\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = criterion(outputs.logits.view(-1, VOCAB_SIZE), labels.view(-1))\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 78/78 [07:14<00:00,  5.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 validation loss: 3.275444278350243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the validation loss for each model\n",
    "\n",
    "gpt2_val_loss = evaluate_model(gpt2_model, val_dataloader)\n",
    "\n",
    "# Print validation loss\n",
    "\n",
    "print(f\"GPT-2 validation loss: {gpt2_val_loss}\")\n",
    "\n",
    "# Calculate the perplexity for each model\n",
    "perplexity = exp(gpt2_val_loss)\n",
    "print(f\"GPT-2 perplexity: {perplexity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 78/78 [01:43<00:00,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM validation loss: 0.060352194481171094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the validation loss for each model\n",
    "\n",
    "lstm_val_loss = evaluate_model(lstm_model, val_dataloader)\n",
    "\n",
    "# Print validation loss\n",
    "\n",
    "print(f\"LSTM validation loss: {lstm_val_loss}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss for the LSTM model is lower than the loss for the GPT-2 Model. This suggests that the LSTM model is better at predicting the next word in the sequence than the GPT-2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 perplexity: 26.454976243265854\n",
      "LSTM perplexity: 1.062210585380299\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Calculate perplexity\n",
    "def calculate_perplexity(loss):\n",
    "    return math.exp(loss)\n",
    "\n",
    "# Calculate perplexity for each model\n",
    "gpt2_perplexity = calculate_perplexity(gpt2_val_loss)\n",
    "lstm_perplexity = calculate_perplexity(lstm_val_loss)\n",
    "\n",
    "print(f\"GPT-2 perplexity: {gpt2_perplexity}\")\n",
    "print(f\"LSTM perplexity: {lstm_perplexity}\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the GPT-2 model, we obtained a perplexity of approximately 26.45, indicating that, on average, the model is predicting the next word with a higher level of uncertainty compared to the ground truth.\n",
    "\n",
    "On the other hand, the LSTM model achieved a perplexity of around 1.06, indicating that it is making more confident predictions and aligning better with the ground truth.\n",
    "\n",
    "Now let's do some manual testing to see how the models behave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thou art thou not God's own God? But thou art God's own God.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the GPT-2 model and tokenizer\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "gpt2_model.eval()\n",
    "\n",
    "# Set the prompt text\n",
    "prompt = \"Thou art\"\n",
    "\n",
    "# Tokenize the prompt\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "# Generate text using the GPT-2 model with temperature\n",
    "output = gpt2_model.generate(input_ids, max_length=20, do_sample=True, top_k=50, temperature=0.5, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# Decode and print the generated text\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blue ocean, and the ocean itself, the ocean is not the same as the ocean of the Sun\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Blue ocean\"\n",
    "# Tokenize the prompt\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "# Generate text using the GPT-2 model with temperature\n",
    "output = gpt2_model.generate(input_ids, max_length=20, do_sample=True, top_k=50, temperature=0.5, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# Decode and print the generated text\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is life? What is the world? What is the Stewart family?\n",
      "\n",
      "What is life\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is life?\"\n",
    "# Tokenize the prompt\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "# Generate text using the GPT-2 model with temperature\n",
    "output = gpt2_model.generate(input_ids, max_length=20, do_sample=True, top_k=50, temperature=0.5, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# Decode and print the generated text\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thou art  art Blocks  inexperienced\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "lstm_model.eval()\n",
    "\n",
    "# Start prompt\n",
    "start_prompt = \"Thou art\"\n",
    "generated = start_prompt\n",
    "\n",
    "# Tokenize the start prompt\n",
    "input_ids = tokenizer.encode(start_prompt, return_tensors='pt').to(device)\n",
    "\n",
    "# Generate three words\n",
    "for _ in range(3):\n",
    "    with torch.no_grad():\n",
    "        outputs = lstm_model(input_ids)\n",
    "        predictions = outputs[0, -1, :]\n",
    "\n",
    "    # Apply softmax to predictions to get probabilities\n",
    "    probabilities = F.softmax(predictions, dim=-1)\n",
    "\n",
    "    # Sample from the distribution\n",
    "    predicted_id = torch.multinomial(probabilities, 1)\n",
    "    generated_word = tokenizer.decode([predicted_id.item()])\n",
    "    generated += \" \" + generated_word\n",
    "\n",
    "    # Prepare the new input\n",
    "    input_ids = predicted_id.unsqueeze(0).to(device)\n",
    "\n",
    "print(generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blue ocean ai  Fritz Brain\n"
     ]
    }
   ],
   "source": [
    "# Start prompt\n",
    "start_prompt = \"Blue ocean\"\n",
    "generated = start_prompt\n",
    "\n",
    "# Tokenize the start prompt\n",
    "input_ids = tokenizer.encode(start_prompt, return_tensors='pt').to(device)\n",
    "\n",
    "# Generate three words\n",
    "for _ in range(3):\n",
    "    with torch.no_grad():\n",
    "        outputs = lstm_model(input_ids)\n",
    "        predictions = outputs[0, -1, :]\n",
    "\n",
    "    # Apply softmax to predictions to get probabilities\n",
    "    probabilities = F.softmax(predictions, dim=-1)\n",
    "\n",
    "    # Sample from the distribution\n",
    "    predicted_id = torch.multinomial(probabilities, 1)\n",
    "    generated_word = tokenizer.decode([predicted_id.item()])\n",
    "    generated += \" \" + generated_word\n",
    "\n",
    "    # Prepare the new input\n",
    "    input_ids = predicted_id.unsqueeze(0).to(device)\n",
    "\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is life? ? ? ?\n"
     ]
    }
   ],
   "source": [
    "# Start prompt\n",
    "start_prompt = \"What is life?\"\n",
    "generated = start_prompt\n",
    "\n",
    "# Tokenize the start prompt\n",
    "input_ids = tokenizer.encode(start_prompt, return_tensors='pt').to(device)\n",
    "\n",
    "# Generate three words\n",
    "for _ in range(3):\n",
    "    with torch.no_grad():\n",
    "        outputs = lstm_model(input_ids)\n",
    "        predictions = outputs[0, -1, :]\n",
    "\n",
    "    # Apply softmax to predictions to get probabilities\n",
    "    probabilities = F.softmax(predictions, dim=-1)\n",
    "\n",
    "    # Sample from the distribution\n",
    "    predicted_id = torch.multinomial(probabilities, 1)\n",
    "    generated_word = tokenizer.decode([predicted_id.item()])\n",
    "    generated += \" \" + generated_word\n",
    "\n",
    "    # Prepare the new input\n",
    "    input_ids = predicted_id.unsqueeze(0).to(device)\n",
    "\n",
    "print(generated)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual testing returns much better results for the GPT-2 model. The LSTM model is more likely to generate words or characters that are not in the vocabulary, while the GPT-2 model is more likely to generate words that are in the vocabulary. The LSTM model seems to generate random words.\n",
    "\n",
    "The GPT-2 model is also better at generating coherent sentences. The LSTM model is more likely to generate sentences that do not make sense.\n",
    "\n",
    "To improve the LSTM model, we could try to increase the number of epochs and the number of hidden layers. We could also try to use a larger vocabulary or dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
